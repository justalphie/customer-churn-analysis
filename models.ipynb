{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import classification_report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training prediction models\n",
    "This section is the most important part of the project it deals with the training of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/BankChurners_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I loaded the data, I have defined the axes and split the test and the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Customer_Age', 'Dependent_count', 'Months_on_book',\n",
      "       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n",
      "       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n",
      "       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n",
      "       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',\n",
      "       'Gender_F', 'Gender_M', 'Education_Level_College',\n",
      "       'Education_Level_Doctorate', 'Education_Level_Graduate',\n",
      "       'Education_Level_High School', 'Education_Level_Post-Graduate',\n",
      "       'Education_Level_Uneducated', 'Education_Level_Unknown',\n",
      "       'Marital_Status_Divorced', 'Marital_Status_Married',\n",
      "       'Marital_Status_Single', 'Marital_Status_Unknown',\n",
      "       'Income_Category_$120K +', 'Income_Category_$40K - $60K',\n",
      "       'Income_Category_$60K - $80K', 'Income_Category_$80K - $120K',\n",
      "       'Income_Category_Less than $40K', 'Income_Category_Unknown',\n",
      "       'Card_Category_Blue', 'Card_Category_Gold', 'Card_Category_Platinum',\n",
      "       'Card_Category_Silver'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'Attrited'], axis=1)\n",
    "y=df[[\"Attrited\"]]\n",
    "print(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, I trained the Decision Tree classifer and evaluated its result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.928923988153998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96      2543\n",
      "         1.0       0.78      0.78      0.78       496\n",
      "\n",
      "    accuracy                           0.93      3039\n",
      "   macro avg       0.87      0.87      0.87      3039\n",
      "weighted avg       0.93      0.93      0.93      3039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is too good! this is because the dataset, including the test set, has mostly existing customers. ```RandomForestClassifier``` is a widely used ML model for its inherent ability to not favor the majority class ([source](https://semaphoreci.com/blog/imbalanced-data-machine-learning-python#:~:text=In%20a%20nutshell%2C%20imbalanced%20data,handling%20the%20less%20common%20cases))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justa\\OneDrive\\Документы\\becode\\customer-churn-analysis\\.venv\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Train Set: 1.0\n",
      "Accuracy on Test Set: 0.9506416584402764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.99      0.97      2543\n",
      "         1.0       0.93      0.75      0.83       496\n",
      "\n",
      "    accuracy                           0.95      3039\n",
      "   macro avg       0.94      0.87      0.90      3039\n",
      "weighted avg       0.95      0.95      0.95      3039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on train and test sets\n",
    "y_train_pred = rf_classifier.predict(X_train)\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy for the train set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Accuracy on Train Set:\", train_accuracy)\n",
    "\n",
    "# Calculate and print accuracy for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Test Set:\", test_accuracy)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives very high scores as well; this means that the data needs to be resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resampling, I train Decision Tree, KNN and Random Forest classifier and print their scores and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Decision Tree classifer: 0.9282658769332017\n",
      "Report for Decision Tree classifer: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96      2543\n",
      "         1.0       0.79      0.77      0.78       496\n",
      "\n",
      "    accuracy                           0.93      3039\n",
      "   macro avg       0.87      0.86      0.87      3039\n",
      "weighted avg       0.93      0.93      0.93      3039\n",
      "\n",
      "Accuracy KNN classifer: 0.8845014807502468\n",
      "Report for KNN classifer: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93      2543\n",
      "         1.0       0.69      0.54      0.60       496\n",
      "\n",
      "    accuracy                           0.88      3039\n",
      "   macro avg       0.80      0.75      0.77      3039\n",
      "weighted avg       0.88      0.88      0.88      3039\n",
      "\n",
      "Accuracy RandomForestClassifier classifer:  0.9506416584402764\n",
      "Report for RandomForestClassifier classifer: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.99      0.97      2543\n",
      "         1.0       0.93      0.75      0.83       496\n",
      "\n",
      "    accuracy                           0.95      3039\n",
      "   macro avg       0.94      0.87      0.90      3039\n",
      "weighted avg       0.95      0.95      0.95      3039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#oversampling after the split\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_oversampled, y_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train.values.ravel())\n",
    "y_pred = clf.predict(X_test)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy Decision Tree classifer:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(f\"Report for Decision Tree classifer: \\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Create KNN classifer object\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf = knn_clf.fit(X_train,y_train.values.ravel())\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy KNN classifer:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(f\"Report for KNN classifer: \\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Create RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_prob = rf_classifier.predict_proba(X_test)\n",
    "print(\"Accuracy RandomForestClassifier classifer: \",metrics.accuracy_score(y_test, y_pred))\n",
    "print(f\"Report for RandomForestClassifier classifer: \\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, Random Forest Classifier is the best model for the task. The results need to be saved for further visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the probabilities into the dataframe and save\n",
    "X_test = X_test.drop(columns=[\"Churning_probability\"], axis=1)\n",
    "y_prob_all = rf_classifier.predict_proba(X_test)\n",
    "\n",
    "df_1 = pd.DataFrame(y_prob_all, index=X_test.index, columns=[\"prob_exist\", \"prob_churn\"])\n",
    "X_test_pred = X_test.copy(deep=True)\n",
    "X_test[\"Churning_probability\"] = df_1[\"prob_churn\"]\n",
    "X_test[\"Attrited\"] = y_test\n",
    "X_test.to_csv(\"data/BankChurners_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy of resampling is undersampling. It removes the data from the larger category (in our case, existing customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Decision Tree classifer: 0.8943731490621916\n",
      "Report for Decision Tree classifer: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.89      0.93      2543\n",
      "         1.0       0.62      0.90      0.74       496\n",
      "\n",
      "    accuracy                           0.89      3039\n",
      "   macro avg       0.80      0.90      0.83      3039\n",
      "weighted avg       0.92      0.89      0.90      3039\n",
      "\n",
      "Accuracy KNN classifer: 0.8180322474498191\n",
      "Report for KNN classifer: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.82      0.88      2543\n",
      "         1.0       0.47      0.80      0.59       496\n",
      "\n",
      "    accuracy                           0.82      3039\n",
      "   macro avg       0.71      0.81      0.74      3039\n",
      "weighted avg       0.87      0.82      0.84      3039\n",
      "\n",
      "Accuracy RandomForestClassifier classifer: 0.9279368213228035\n",
      "Report for RandomForestClassifier classifer: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.92      0.96      2543\n",
      "         1.0       0.71      0.94      0.81       496\n",
      "\n",
      "    accuracy                           0.93      3039\n",
      "   macro avg       0.85      0.93      0.88      3039\n",
      "weighted avg       0.94      0.93      0.93      3039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#undersampling\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train, y_train= undersampler.fit_resample(X_train, y_train)\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train.values.ravel())\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy Decision Tree classifer:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(f\"Report for Decision Tree classifer: \\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Create KNN classifer object\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf = knn_clf.fit(X_train,y_train.values.ravel())\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy KNN classifer:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(f\"Report for KNN classifer: \\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#Create RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(\"Accuracy RandomForestClassifier classifer:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(f\"Report for RandomForestClassifier classifer: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling has shown better results than undersampling, therefore, this type of preprocessing is preferred for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_Trans_Amt                   0.197753\n",
      "Total_Trans_Ct                    0.193131\n",
      "Total_Revolving_Bal               0.102252\n",
      "Total_Ct_Chng_Q4_Q1               0.096499\n",
      "Avg_Utilization_Ratio             0.062177\n",
      "Total_Amt_Chng_Q4_Q1              0.059250\n",
      "Total_Relationship_Count          0.041480\n",
      "Credit_Limit                      0.035053\n",
      "Avg_Open_To_Buy                   0.032530\n",
      "Months_Inactive_12_mon            0.031252\n",
      "Customer_Age                      0.024953\n",
      "Months_on_book                    0.022667\n",
      "Contacts_Count_12_mon             0.021360\n",
      "Dependent_count                   0.011781\n",
      "Gender_M                          0.006396\n",
      "Gender_F                          0.006390\n",
      "Marital_Status_Married            0.006226\n",
      "Education_Level_Graduate          0.004913\n",
      "Marital_Status_Single             0.004109\n",
      "Income_Category_$80K - $120K      0.003541\n",
      "Education_Level_Unknown           0.003205\n",
      "Income_Category_$60K - $80K       0.003200\n",
      "Education_Level_High School       0.003164\n",
      "Income_Category_Less than $40K    0.002957\n",
      "Education_Level_Post-Graduate     0.002765\n",
      "Income_Category_$40K - $60K       0.002615\n",
      "Education_Level_College           0.002470\n",
      "Education_Level_Uneducated        0.002302\n",
      "Income_Category_$120K +           0.002169\n",
      "Education_Level_Doctorate         0.002129\n",
      "Income_Category_Unknown           0.002126\n",
      "Marital_Status_Divorced           0.001807\n",
      "Card_Category_Blue                0.001684\n",
      "Marital_Status_Unknown            0.001604\n",
      "Card_Category_Silver              0.001318\n",
      "Card_Category_Gold                0.000621\n",
      "Card_Category_Platinum            0.000153\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_scores = pd.Series(rf_classifier.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(feature_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features are _Total_Trans_Amt (0.197753), Total_Trans_Ct (0.193131), Total_Revolving_Bal, (0.102252)._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
